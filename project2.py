# -*- coding: utf-8 -*-
"""Project2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jPiBU3BIqwi_yCb2R9qZhZdb9TgrJxSU
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load the dataset
data = pd.read_csv('household_power_consumption.txt', sep=';', parse_dates={'datetime': ['Date', 'Time']}, infer_datetime_format=True, low_memory=False, na_values=['nan','?'])

# Handle missing values
data.fillna(method='ffill', inplace=True)

# Set datetime index
data.set_index('datetime', inplace=True)

# Convert data types
data = data.astype({'Global_active_power': 'float64', 'Global_reactive_power': 'float64',
                    'Voltage': 'float64', 'Global_intensity': 'float64',
                    'Sub_metering_1': 'float64', 'Sub_metering_2': 'float64',
                    'Sub_metering_3': 'float64'})

# Resample data to daily frequency
daily_data = data.resample('D').mean()

# Scale data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(daily_data['Global_active_power'].values.reshape(-1, 1))

# Convert to supervised learning problem
def create_dataset(dataset, look_back=1):
    X, y = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 1
X, y = create_dataset(scaled_data, look_back)

# Split into train and test sets
train_size = int(len(X) * 0.8)
train_X, test_X = X[:train_size], X[train_size:]
train_y, test_y = y[:train_size], y[train_size:]

# Reshape input to be [samples, time steps, features]
train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))
test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))

# Build LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(1, look_back)))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
model.fit(train_X, train_y, epochs=100, batch_size=1, verbose=2)

# Make predictions
train_predict = model.predict(train_X)
test_predict = model.predict(test_X)

# Inverse transform predictions
train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)

# Inverse transform true values
train_y = scaler.inverse_transform([train_y])
test_y = scaler.inverse_transform([test_y])

# Evaluate the model
train_score = np.sqrt(mean_squared_error(train_y[0], train_predict[:,0]))
test_score = np.sqrt(mean_squared_error(test_y[0], test_predict[:,0]))
print(f'Train Score: {train_score} RMSE')
print(f'Test Score: {test_score} RMSE')

# Plot the results
train_predict_plot = np.empty_like(scaled_data)
train_predict_plot[:, :] = np.nan
train_predict_plot[look_back:len(train_predict)+look_back, :] = train_predict

test_predict_plot = np.empty_like(scaled_data)
test_predict_plot[:, :] = np.nan
test_predict_plot[len(train_predict)+(look_back*2)+1:len(scaled_data)-1, :] = test_predict

plt.figure(figsize=(14,7))
plt.plot(scaler.inverse_transform(scaled_data), label='True Data')
plt.plot(train_predict_plot, label='Train Predict')
plt.plot(test_predict_plot, label='Test Predict')
plt.title('LSTM Model - Global Active Power')
plt.xlabel('Date')
plt.ylabel('Global Active Power (kilowatts)')
plt.legend()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load the dataset
data = pd.read_csv('household_power_consumption.txt', sep=';', parse_dates={'datetime': ['Date', 'Time']}, infer_datetime_format=True, low_memory=False, na_values=['nan','?'])

# Handle missing values
data.fillna(method='ffill', inplace=True)

# Set datetime index
data.set_index('datetime', inplace=True)

# Convert data types
data = data.astype({'Global_active_power': 'float64', 'Global_reactive_power': 'float64',
                    'Voltage': 'float64', 'Global_intensity': 'float64',
                    'Sub_metering_1': 'float64', 'Sub_metering_2': 'float64',
                    'Sub_metering_3': 'float64'})

# Resample data to daily frequency
daily_data = data.resample('D').mean()

# Scale data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(daily_data['Global_active_power'].values.reshape(-1, 1))

# Convert to supervised learning problem
def create_dataset(dataset, look_back=1):
    X, y = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        X.append(a)
        y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(y)

look_back = 1
X, y = create_dataset(scaled_data, look_back)

# Split into train and test sets
train_size = int(len(X) * 0.8)
train_X, test_X = X[:train_size], X[train_size:]
train_y, test_y = y[:train_size], y[train_size:]

# Reshape input to be [samples, time steps, features]
train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))
test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))

# Build LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(1, look_back)))
model.add(LSTM(50))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
model.fit(train_X, train_y, epochs=100, batch_size=1, verbose=2)

train_predict = model.predict(train_X)
test_predict = model.predict(test_X)

train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)

train_y = scaler.inverse_transform([train_y])
test_y = scaler.inverse_transform([test_y])
train_score = np.sqrt(mean_squared_error(train_y[0], train_predict[:,0]))
test_score = np.sqrt(mean_squared_error(test_y[0], test_predict[:,0]))
print(f'Train Score: {train_score} RMSE')
print(f'Test Score: {test_score} RMSE')
train_predict_plot = np.empty_like(scaled_data)
train_predict_plot[:, :] = np.nan
train_predict_plot[look_back:len(train_predict) + look_back, :] = train_predict

test_predict_plot = np.empty_like(scaled_data)
test_predict_plot[:, :] = np.nan
test_predict_plot[len(train_predict) + (look_back * 2):len(train_predict) + (look_back * 2) + len(test_predict), :] = test_predict

plt.figure(figsize=(14, 7))
plt.plot(scaler.inverse_transform(scaled_data), label='True Data')
plt.plot(train_predict_plot, label='Train Predict')
plt.plot(test_predict_plot, label='Test Predict')
plt.title('LSTM Model - Global Active Power')
plt.xlabel('Date')
plt.ylabel('Global Active Power (kilowatts)')
plt.legend()
plt.show()